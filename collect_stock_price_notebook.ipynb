{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true,
				"tags": [],
				"collapsed": true,
				"jupyter": {
					"outputs_hidden": true
				}
			},
			"execution_count": 10,
			"outputs": [
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %timeout            Int           The number of minutes after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session.\n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n    %reconnect          String        Specify a live session ID to switch/reconnect to the sessions.\n----\n\n## Selecting Session Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n    %session_type       String        Specify a session_type to be used. Supported values: streaming, etl and glue_ray. \n----\n\n## Glue Config Magic \n*(common across all session types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n    \n    %%assume_role Dictionary, String  Specify a json-formatted dictionary or an IAM role ARN string to create a session \n                                      for cross account access.\n                                      E.g. {valid arn}\n                                      %%assume_role \n                                      'arn:aws:iam::XXXXXXXXXXXX:role/AWSGlueServiceRole' \n                                      E.g. {credentials}\n                                      %%assume_role\n                                      {\n                                            \"aws_access_key_id\" : \"XXXXXXXXXXXX\",\n                                            \"aws_secret_access_key\" : \"XXXXXXXXXXXX\",\n                                            \"aws_session_token\" : \"XXXXXXXXXXXX\"\n                                       }\n----\n\n                                      \n## Magic for Spark Sessions (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Session\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray session. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n    %matplot      Matplotlib figure   Visualize your data using the matplotlib library.\n                                      E.g. \n                                      import matplotlib.pyplot as plt\n                                      # Set X-axis and Y-axis values\n                                      x = [5, 2, 8, 4, 9]\n                                      y = [10, 4, 8, 5, 2]\n                                      # Create a bar chart \n                                      plt.bar(x, y) \n                                      # Show the plot\n                                      %matplot plt    \n    %plotly            Plotly figure  Visualize your data using the plotly library.\n                                      E.g.\n                                      import plotly.express as px\n                                      #Create a graphical figure\n                                      fig = px.line(x=[\"a\",\"b\",\"c\"], y=[1,3,2], title=\"sample figure\")\n                                      #Show the figure\n                                      %plotly fig\n\n  \n                \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# yfinance 모듈 추가\n%additional_python_modules yfinance==0.2.48",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nAdditional python modules to be included:\nyfinance==0.2.48\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nimport pandas as pd\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nimport yfinance as yf\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: 273ee12e-e078-4f41-90d5-a6949c022813\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\n--additional-python-modules yfinance==0.2.48\nWaiting for session 273ee12e-e078-4f41-90d5-a6949c022813 to get into ready status...\nSession 273ee12e-e078-4f41-90d5-a6949c022813 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# 수집할 종목 리스트 (예시: SK하이닉스)\nsymbol = '000660.KS'\nticker = yf.Ticker(symbol)\n# 오늘부터 5년전 데이터 조회\nfrom datetime import datetime\n\nstartMd = '01-01'\nendMd = '12-31'\ndfs = []\n\n# 현재 연도를 가져와서 5년 전부터 현재 연도까지 반복\ncurrent_year = datetime.now().year\n\nfor year in range(current_year - 5, current_year + 1):\n    # 연도별 데이터 조회\n    df_year = ticker.history(interval='1d', period='1y', start=f\"{year}-{startMd}\", end=f\"{year}-{endMd}\", auto_adjust=False)\n    dfs.append(df_year)\n\ndf = pd.concat(dfs)\ndf.reset_index(inplace=True)\n\n# pandas Dataframe을 Spark DataFrame으로 변환\nspark_df = spark.createDataFrame(df)\nspark_df.show()\nspark_df.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+-------+-------+-------+-------+--------------+-------+---------+------------+\n|               Date|   Open|   High|    Low|  Close|     Adj Close| Volume|Dividends|Stock Splits|\n+-------------------+-------+-------+-------+-------+--------------+-------+---------+------------+\n|2019-01-01 15:00:00|61300.0|61400.0|60000.0|60600.0| 56981.5234375|1934295|      0.0|         0.0|\n|2019-01-02 15:00:00|60200.0|60300.0|57500.0|57700.0|54254.68359375|3337621|      0.0|         0.0|\n|2019-01-03 15:00:00|57500.0|58500.0|56700.0|58300.0|  54818.859375|3388087|      0.0|         0.0|\n|2019-01-06 15:00:00|59000.0|59700.0|58400.0|58700.0|55194.96484375|2273750|      0.0|         0.0|\n|2019-01-07 15:00:00|57900.0|60500.0|57600.0|59200.0| 55665.1171875|3062192|      0.0|         0.0|\n|2019-01-08 15:00:00|59600.0|63800.0|59400.0|63600.0|59802.38671875|4928656|      0.0|         0.0|\n|2019-01-09 15:00:00|64100.0|65600.0|64000.0|65300.0|61400.87890625|6034432|      0.0|         0.0|\n|2019-01-10 15:00:00|65400.0|65900.0|64200.0|65100.0| 61212.8203125|2890639|      0.0|         0.0|\n|2019-01-13 15:00:00|64300.0|65100.0|62000.0|62100.0|58391.94921875|3282892|      0.0|         0.0|\n|2019-01-14 15:00:00|62200.0|64000.0|62200.0|64000.0|       60178.5|2413868|      0.0|         0.0|\n|2019-01-15 15:00:00|63800.0|64900.0|62700.0|64800.0|  60930.734375|2478497|      0.0|         0.0|\n|2019-01-16 15:00:00|65500.0|66200.0|64600.0|64900.0| 61024.7578125|2830122|      0.0|         0.0|\n|2019-01-17 15:00:00|65000.0|65700.0|63900.0|64600.0|60742.68359375|2263599|      0.0|         0.0|\n|2019-01-20 15:00:00|65900.0|67400.0|65000.0|67000.0|62999.37109375|3977425|      0.0|         0.0|\n|2019-01-21 15:00:00|67900.0|68000.0|65700.0|66600.0|62623.25390625|4047917|      0.0|         0.0|\n|2019-01-22 15:00:00|65500.0|67200.0|64800.0|66800.0|62811.31640625|2825017|      0.0|         0.0|\n|2019-01-23 15:00:00|66900.0|70500.0|66900.0|70500.0|     66290.375|6914092|      0.0|         0.0|\n|2019-01-24 15:00:00|71600.0|74800.0|71100.0|74600.0|  70145.578125|7163871|      0.0|         0.0|\n|2019-01-27 15:00:00|74700.0|74900.0|71200.0|71800.0| 67512.7578125|4972189|      0.0|         0.0|\n|2019-01-28 15:00:00|71400.0|73800.0|71000.0|73400.0| 69017.2109375|3368642|      0.0|         0.0|\n+-------------------+-------+-------+-------+-------+--------------+-------+---------+------------+\nonly showing top 20 rows\n\nroot\n |-- Date: timestamp (nullable = true)\n |-- Open: double (nullable = true)\n |-- High: double (nullable = true)\n |-- Low: double (nullable = true)\n |-- Close: double (nullable = true)\n |-- Adj Close: double (nullable = true)\n |-- Volume: long (nullable = true)\n |-- Dividends: double (nullable = true)\n |-- Stock Splits: double (nullable = true)\n\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# S3에 parquet 형식으로 저장\ns3_output_path = 's3://jackie-python-lib/stock_data/'\nfinal_path = f\"{s3_output_path}{symbol}\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# parquet 형식으로 저장\nspark_df.write.mode('overwrite').parquet(final_path)\n\nprint(f\"데이터 저장 완료: {final_path}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "데이터 저장 완료: s3://jackie-python-lib/stock_data/000660.KS\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# 종료\njob.commit()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		}
	]
}